{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "path = \"C:/Users/varun/Desktop/GR 10 sec ad.mp4\"\n",
    "capture = cv2.VideoCapture(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while (True):\n",
    "    success, frame = capture.read()\n",
    "    if success:\n",
    "        # print('Valid Video')\n",
    "        cv2.imwrite(f'C:/Users/varun/Desktop/computer vision/output/image_{count}.jpg', frame)\n",
    "    else:\n",
    "        # print('No frames captured')\n",
    "        break\n",
    " \n",
    "    count += 1\n",
    " \n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_image_time(path, capture_interval = 1):\n",
    "    # Open the video capture object\n",
    "    os.mkdir('C:/Users/varun/Desktop/computer vision/output1') \n",
    "    capture = cv2.VideoCapture(path)\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"total_frames : \", total_frames)\n",
    "\n",
    "    # Calculate the desired number of frames based on the capture interval\n",
    "    desired_frames = int(total_frames / (capture.get(cv2.CAP_PROP_FPS) * capture_interval))\n",
    "    print(\"desired_frames : \", desired_frames)\n",
    "\n",
    "    count = 0\n",
    "    while count < desired_frames:\n",
    "        # Set the frame index based on the capture interval\n",
    "        frame_index = int(count * capture.get(cv2.CAP_PROP_FPS) * capture_interval)\n",
    "        # print(\"frame_index : \", frame_index)\n",
    "        \n",
    "        # Set the frame position    \n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "        success, frame = capture.read()\n",
    "\n",
    "        if success:\n",
    "            cv2.imwrite(f'C:/Users/varun/Desktop/computer vision/output1/image_{count}.jpg', frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use capture.get(cv2.CAP_PROP_POS_FRAMES), you are retrieving the current frame index in the video stream. Similarly, when you use capture.set(cv2.CAP_PROP_POS_FRAMES, frame_index), you are setting the position in the video stream to the specified frame index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/varun/Desktop/GR 10 sec ad.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_image(path, desired_fps = 5): \n",
    "    capture = cv2.VideoCapture(path)\n",
    "    os.mkdir('C:/Users/varun/Desktop/computer vision/output') \n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"total_frames\", total_frames)\n",
    "    print(\"capture.get(cv2.CAP_PROP_FPS)\", capture.get(cv2.CAP_PROP_FPS))\n",
    "    # Calculate the frame skip factor based on the desired FPS\n",
    "    frame_skip = int(capture.get(cv2.CAP_PROP_FPS) / desired_fps)\n",
    "    print(\"frame_skip\", frame_skip)\n",
    "\n",
    "    count = 0\n",
    "    while count < total_frames:\n",
    "        # Read the frame\n",
    "        success, frame = capture.read()\n",
    "\n",
    "        if success:\n",
    "            # print(f'{path} + {count}.jpg')\n",
    "            cv2.imwrite(f'C:/Users/varun/Desktop/computer vision/output/image_{count}.jpg', frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Skip frames to achieve the desired FPS\n",
    "        for _ in range(frame_skip - 1):\n",
    "            capture.read()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    capture.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frames 300\n",
      "capture.get(cv2.CAP_PROP_FPS) 29.97002997002997\n",
      "frame_skip 5\n"
     ]
    }
   ],
   "source": [
    "video_to_image(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_video(path, desired_fps = 5):\n",
    "    image_folder = 'C:/Users/varun/Desktop/computer vision/output/'\n",
    "\n",
    "    # Path to the output video file\n",
    "    video_output_path = 'C:/Users/varun/Desktop/computer vision/output_video.avi'\n",
    "\n",
    "    # Get the list of image files in the folder\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "\n",
    "    # Sort the images based on their names\n",
    "    images.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "\n",
    "    # Get the first image to get the size\n",
    "    img = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = img.shape\n",
    "\n",
    "    # Create a VideoWriter object to save the video\n",
    "    video = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*'DIVX'), desired_fps, (width, height))\n",
    "\n",
    "    # Loop through the images and write each frame to the video\n",
    "    for image in images:\n",
    "        img = cv2.imread(os.path.join(image_folder, image))\n",
    "        video.write(img)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved at C:/Users/varun/Desktop/computer vision/output_video.avi\n"
     ]
    }
   ],
   "source": [
    "image_to_video(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chroma-keying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_foreground = \"C:/Users/varun/Desktop/green_tiger.mp4\"\n",
    "path_background = \"C:/Users/varun/Desktop/nature.mp4\"\n",
    "\n",
    "capture_foreground = cv2.VideoCapture(path_foreground)\n",
    "capture_background = cv2.VideoCapture(path_background)\n",
    "count = 0\n",
    "while (True):\n",
    "    success_foreground, frame_foreground = capture_foreground.read()\n",
    "    success_background, frame_background = capture_background.read()\n",
    "    if success_foreground and success_background:\n",
    "        # cv2.imwrite(f'C:/Users/varun/Desktop/computer vision/output/image_{count}.jpg', frame)\n",
    "        frame_foreground = \n",
    "        frame_background = \n",
    "    \n",
    "    else:\n",
    "        break\n",
    " \n",
    "    count += 1\n",
    " \n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video capture objects for both the main video and scenery video\n",
    "main_video_path = \"C:/Users/varun/Desktop/green_tiger.mp4\"\n",
    "scenery_video_path = \"C:/Users/varun/Desktop/scenery.mp4\"\n",
    "\n",
    "main_capture = cv2.VideoCapture(main_video_path)\n",
    "scenery_capture = cv2.VideoCapture(scenery_video_path)\n",
    "\n",
    "# Define the range of green color in HSV\n",
    "lower_green = np.array([40, 40, 40])\n",
    "upper_green = np.array([80, 255, 255])\n",
    "\n",
    "# Create a VideoWriter object for the output video\n",
    "output_path = 'C:/Users/varun/Desktop/computer vision/output_with_scenery.avi'\n",
    "fps = main_capture.get(cv2.CAP_PROP_FPS)\n",
    "width = int(main_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(main_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    # Read frames from both videos\n",
    "    main_ret, main_frame = main_capture.read()\n",
    "    scenery_ret, scenery_frame = scenery_capture.read()\n",
    "\n",
    "    # Break the loop if either of the videos ends\n",
    "    if not main_ret or not scenery_ret:\n",
    "        break\n",
    "\n",
    "    # Convert the main video frame from BGR to HSV color space\n",
    "    hsv = cv2.cvtColor(main_frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask based on the green color range\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "    # Morphological operations to remove noise and fill gaps in the mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Invert the mask to get the foreground\n",
    "    foreground = cv2.bitwise_and(main_frame, main_frame, mask=~mask)\n",
    "\n",
    "    # Resize the scenery frame to match the main video frame\n",
    "    scenery_frame = cv2.resize(scenery_frame, (width, height))\n",
    "\n",
    "    # Create a mask for the green screen area\n",
    "    green_screen_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Extract the scenery from the scenery frame using the mask\n",
    "    scenery_on_green = cv2.bitwise_and(scenery_frame, scenery_frame, mask=green_screen_mask)\n",
    "\n",
    "    # Combine the foreground (main video without green screen) and scenery\n",
    "    result = cv2.bitwise_or(foreground, scenery_on_green)\n",
    "\n",
    "    # Display the result or write it to the output video\n",
    "    output_video.write(result)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "main_capture.release()\n",
    "scenery_capture.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the green screen video capture object\n",
    "green_screen_path = \"C:/Users/varun/Desktop/green_tiger.mp4\"\n",
    "capture_foreground = cv2.VideoCapture(green_screen_path)\n",
    "\n",
    "# Define the range of green color in HSV\n",
    "lower_green = np.array([40, 40, 40])\n",
    "upper_green = np.array([80, 255, 255])\n",
    "\n",
    "# Create a VideoWriter object for the output video\n",
    "output_path = 'C:/Users/varun/Desktop/computer vision/output_with_clear1.avi'\n",
    "fps = capture_foreground.get(cv2.CAP_PROP_FPS)\n",
    "width = int(capture_foreground.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture_foreground.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Open the scenery video capture object\n",
    "scenery_path = \"C:/Users/varun/Desktop/nature.mp4\"  \n",
    "capture_background = cv2.VideoCapture(scenery_path)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the green screen video\n",
    "    success_foreground, frame_foreground = capture_foreground.read()\n",
    "    success_background, frame_background = capture_background.read()\n",
    "\n",
    "    success_foreground, frame_foreground = capture_foreground.read()\n",
    "    success_background, frame_background = capture_background.read()\n",
    "    if success_background and success_foreground:\n",
    "        # Convert the frame from BGR to HSV color space\n",
    "        hsv = cv2.cvtColor(frame_foreground, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Create a mask based on the green color range\n",
    "        mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "        # Morphological operations to remove noise and fill gaps in the mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Invert the mask to get the foreground\n",
    "        foreground = cv2.bitwise_and(frame_foreground, frame_foreground, mask=~mask)\n",
    "\n",
    "        # Resize the scenery frame to match the green screen frame\n",
    "        frame_background = cv2.resize(frame_background, (width, height))\n",
    "\n",
    "        # Replace the green screen region with the scenery frame\n",
    "        result = cv2.bitwise_and(frame_background, frame_background, mask=mask)\n",
    "        result = cv2.add(result, foreground)\n",
    "\n",
    "        # Display the result or write it to the output video\n",
    "        output_video.write(result)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "capture_foreground.release()\n",
    "capture_background.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Open the green screen video capture object\n",
    "# green_screen_path = \"C:/Users/varun/Desktop/green_tiger.mp4\"\n",
    "# capture_foreground = cv2.VideoCapture(green_screen_path)\n",
    "\n",
    "# # Define the range of green color in HSV\n",
    "# lower_green = np.array([40, 40, 40])\n",
    "# upper_green = np.array([80, 255, 255])\n",
    "\n",
    "# # Create a VideoWriter object for the output video\n",
    "# output_path = 'C:/Users/varun/Desktop/computer vision/output_with_clear1.avi'\n",
    "# fps = capture_foreground.get(cv2.CAP_PROP_FPS)\n",
    "# width = int(capture_foreground.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(capture_foreground.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "# output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# # Open the scenery video capture object\n",
    "# scenery_path = \"C:/Users/varun/Desktop/nature.mp4\"  \n",
    "# capture_background = cv2.VideoCapture(scenery_path)\n",
    "\n",
    "# while True:\n",
    "#     # Read a frame from the green screen video\n",
    "#     success_foreground, frame_foreground = capture_foreground.read()\n",
    "#     success_background, frame_background = capture_background.read()\n",
    "\n",
    "#     success_foreground, frame_foreground = capture_foreground.read()\n",
    "#     success_background, frame_background = capture_background.read()\n",
    "#     if success_background and success_foreground:\n",
    "#         # Convert the frame from BGR to HSV color space\n",
    "#         # print(\"frame_foreground\", frame_foreground.shape)\n",
    "\n",
    "#         hsv = cv2.cvtColor(frame_foreground, cv2.COLOR_BGR2HSV)\n",
    "#         print(\"HSV\", hsv.shape)\n",
    "#         print(\"HSV\", hsv[2])\n",
    "#         # Create a mask based on the green color range\n",
    "#         mask = cv2.inRange(frame_foreground, lower_green, upper_green)\n",
    "\n",
    "#         # Morphological operations to remove noise and fill gaps in the mask\n",
    "#         kernel = np.ones((5, 5), np.uint8)\n",
    "#         mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "#         mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "#         # Invert the mask to get the foreground\n",
    "#         foreground = cv2.bitwise_and(frame_foreground, frame_foreground, mask=~mask)\n",
    "\n",
    "#         # Resize the scenery frame to match the green screen frame\n",
    "#         frame_background = cv2.resize(frame_background, (width, height))\n",
    "\n",
    "#         # Replace the green screen region with the scenery frame\n",
    "#         result = cv2.bitwise_and(frame_background, frame_background, mask=mask)\n",
    "#         result = cv2.add(result, foreground)\n",
    "\n",
    "#         # Write it to the output video\n",
    "#         output_video.write(result)\n",
    "#         break\n",
    "\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "#     break\n",
    "# # Release the video capture and writer objects\n",
    "# capture_foreground.release()\n",
    "# capture_background.release()\n",
    "# output_video.release()\n",
    "\n",
    "# # Close all OpenCV windows\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = 'C:/Users/varun/Desktop/computer vision/output/image_43.jpg'\n",
    "image = cv2.imread(input_image_path)\n",
    "\n",
    "# Convert the image to grayscale for face detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Draw rectangles around the detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# Save the resulting image with bounding boxes\n",
    "output_image_path = 'path/to/your/output/image_with_boxes.jpg'\n",
    "cv2.imwrite(output_image_path, image)\n",
    "\n",
    "# Display the resulting image (optional)\n",
    "cv2.imshow('Image with Bounding Boxes', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_path = 'path/to/your/video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video details\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_skip = 5  # Adjust the frame skip as needed\n",
    "\n",
    "# Create a VideoWriter object for the output video\n",
    "output_path = 'path/to/your/output/video_with_boxes.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Process each frame of the video\n",
    "for frame_number in range(total_frames):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Skip frames based on the frame_skip value\n",
    "    if frame_number % frame_skip == 0:\n",
    "        # Convert the frame to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform face detection\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Draw rectangles around the detected faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with bounding boxes to the output video\n",
    "        output_video.write(frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def capture_frames(output_folder): \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not os.path.exists(output_folder): \n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frames = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames += 1\n",
    "        image_path = os.path.join(output_folder, f\"frame_{frames:04d}.jpg\") \n",
    "        cv2.imwrite(image_path, frame)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.waitKey(1)\n",
    "        if frames >= 200: \n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllwindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
